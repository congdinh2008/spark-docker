{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3ef27d",
   "metadata": {},
   "source": [
    "# Apache Spark Docker Cluster Demo\n",
    "\n",
    "**Professional demonstration of Apache Spark running on Docker with comprehensive event logging**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Overview\n",
    "\n",
    "This notebook demonstrates a complete Apache Spark setup with:\n",
    "\n",
    "- **Docker Cluster Mode**: Spark master and workers running in containers\n",
    "- **Event Logging**: Comprehensive tracking of all Spark operations\n",
    "- **Advanced Analytics**: Real-world data processing examples\n",
    "- **Monitoring Integration**: Web UIs for cluster and application monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ï¸ Architecture\n",
    "\n",
    "### Cluster Components\n",
    "- **Spark Master**: Cluster coordinator (port 7077, UI port 8080)\n",
    "- **Spark Workers**: Execution nodes \n",
    "- **History Server**: Event log analysis (port 18080)\n",
    "- **JupyterLab**: Alternative notebook interface (port 8888)\n",
    "\n",
    "### Driver Configuration\n",
    "- **External Driver**: VS Code notebook connects to Docker cluster\n",
    "- **Event Storage**: Local filesystem shared with containers\n",
    "- **Network**: `host.docker.internal` for cross-container communication\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Monitoring Dashboard URLs\n",
    "\n",
    "| Service | URL | Description |\n",
    "|---------|-----|-------------|\n",
    "| Cluster Master | http://localhost:8080 | Live cluster status |\n",
    "| Application UI | http://host.docker.internal:4040 | Current job details |\n",
    "| History Server | http://localhost:18080 | Historical event logs |\n",
    "| JupyterLab | http://localhost:8888 | Container notebook interface |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Quick Start\n",
    "\n",
    "1. **Start containers**: `docker compose up -d`\n",
    "2. **Run cells below**: Execute the Spark configuration and analytics\n",
    "3. **Monitor execution**: Use the dashboard URLs above\n",
    "4. **View event logs**: Check History Server for detailed execution logs\n",
    "\n",
    "---\n",
    "\n",
    "## \udcdd Technical Specifications\n",
    "\n",
    "- **Spark Version**: 3.5.0\n",
    "- **Event Compression**: zstd algorithm\n",
    "- **Serialization**: Kryo for performance\n",
    "- **Memory**: 1GB driver + 1GB executor\n",
    "- **Adaptive Query Execution**: Enabled\n",
    "- **Event Directory**: `./events/` (shared volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e6e58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ³ Initializing Spark Docker Cluster Connection\n",
      "============================================================\n",
      "ğŸ“ Events Directory: /Users/congdinh/Downloads/work/content/de/spark-docker/events\n",
      "â° Session Start: 2025-09-03 21:41:51\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SPARK CLUSTER CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"ğŸ³ Initializing Spark Docker Cluster Connection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "current_dir = os.getcwd()\n",
    "workspace_root = os.path.dirname(current_dir)\n",
    "events_dir = os.path.join(workspace_root, \"events\")\n",
    "events_uri = f\"file://{events_dir}\"\n",
    "\n",
    "# Ensure events directory exists\n",
    "os.makedirs(events_dir, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“ Events Directory: {events_dir}\")\n",
    "print(f\"â° Session Start: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d1b3d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Creating Spark Session...\n",
      "âœ… Spark Session Created Successfully!\n",
      "ğŸ“‹ Application ID: app-20250903144158-0016\n",
      "ğŸ¯ Master: spark://localhost:7077\n",
      "ğŸŒ Application UI: http://host.docker.internal:4040\n",
      "ğŸ“ Event Logging: true\n",
      "ğŸ“ Event Directory: file:///Users/congdinh/Downloads/work/content/de/spark-docker/events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/03 21:41:58 WARN Utils: Service 'sparkDriver' could not bind on port 7078. Attempting port 7079.\n",
      "25/09/03 21:41:58 WARN Utils: Service 'sparkDriver' could not bind on port 7079. Attempting port 7080.\n",
      "25/09/03 21:41:58 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 7079. Attempting port 7080.\n",
      "25/09/03 21:41:58 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 7080. Attempting port 7081.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/03 21:42:14 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SPARK SESSION CREATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸš€ Creating Spark Session...\")\n",
    "\n",
    "# Build Spark session with optimized configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DockerClusterDemo\") \\\n",
    "    .master(\"spark://localhost:7077\") \\\n",
    "    .config(\"spark.driver.host\", \"host.docker.internal\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.driver.port\", \"7078\") \\\n",
    "    .config(\"spark.blockManager.port\", \"7079\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "    .config(\"spark.eventLog.dir\", events_uri) \\\n",
    "    .config(\"spark.eventLog.compress\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Display session information\n",
    "print(\"âœ… Spark Session Created Successfully!\")\n",
    "print(f\"ğŸ“‹ Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"ğŸ¯ Master: {spark.sparkContext.master}\")\n",
    "print(f\"ğŸŒ Application UI: {spark.sparkContext.uiWebUrl}\")\n",
    "print(f\"ğŸ“ Event Logging: {spark.sparkContext.getConf().get('spark.eventLog.enabled')}\")\n",
    "print(f\"ğŸ“ Event Directory: {events_uri}\")\n",
    "\n",
    "# Set log level to reduce noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee52d7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Creating Sample Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset Created: 10 employees\n",
      "ğŸ“‹ Sample Data:\n",
      "+-------------+---+-----------+------+-------+\n",
      "|Name         |Age|Department |Salary|Level  |\n",
      "+-------------+---+-----------+------+-------+\n",
      "|Alice Johnson|28 |Engineering|75000 |Senior |\n",
      "|Bob Smith    |35 |Sales      |65000 |Manager|\n",
      "|Catherine Lee|32 |Marketing  |70000 |Senior |\n",
      "|David Brown  |29 |Engineering|80000 |Senior |\n",
      "|Eva Wilson   |31 |Sales      |68000 |Senior |\n",
      "|Frank Miller |26 |Engineering|72000 |Junior |\n",
      "|Grace Davis  |27 |Marketing  |69000 |Junior |\n",
      "|Henry Taylor |34 |Sales      |71000 |Manager|\n",
      "|Ivy Chen     |30 |Engineering|85000 |Senior |\n",
      "|Jack Anderson|25 |Marketing  |63000 |Junior |\n",
      "+-------------+---+-----------+------+-------+\n",
      "\n",
      "+-------------+---+-----------+------+-------+\n",
      "|Name         |Age|Department |Salary|Level  |\n",
      "+-------------+---+-----------+------+-------+\n",
      "|Alice Johnson|28 |Engineering|75000 |Senior |\n",
      "|Bob Smith    |35 |Sales      |65000 |Manager|\n",
      "|Catherine Lee|32 |Marketing  |70000 |Senior |\n",
      "|David Brown  |29 |Engineering|80000 |Senior |\n",
      "|Eva Wilson   |31 |Sales      |68000 |Senior |\n",
      "|Frank Miller |26 |Engineering|72000 |Junior |\n",
      "|Grace Davis  |27 |Marketing  |69000 |Junior |\n",
      "|Henry Taylor |34 |Sales      |71000 |Manager|\n",
      "|Ivy Chen     |30 |Engineering|85000 |Senior |\n",
      "|Jack Anderson|25 |Marketing  |63000 |Junior |\n",
      "+-------------+---+-----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ“Š Creating Sample Dataset...\")\n",
    "\n",
    "# Sample employee data\n",
    "employee_data = [\n",
    "    (\"Alice Johnson\", 28, \"Engineering\", 75000, \"Senior\"),\n",
    "    (\"Bob Smith\", 35, \"Sales\", 65000, \"Manager\"),\n",
    "    (\"Catherine Lee\", 32, \"Marketing\", 70000, \"Senior\"),\n",
    "    (\"David Brown\", 29, \"Engineering\", 80000, \"Senior\"),\n",
    "    (\"Eva Wilson\", 31, \"Sales\", 68000, \"Senior\"),\n",
    "    (\"Frank Miller\", 26, \"Engineering\", 72000, \"Junior\"),\n",
    "    (\"Grace Davis\", 27, \"Marketing\", 69000, \"Junior\"),\n",
    "    (\"Henry Taylor\", 34, \"Sales\", 71000, \"Manager\"),\n",
    "    (\"Ivy Chen\", 30, \"Engineering\", 85000, \"Senior\"),\n",
    "    (\"Jack Anderson\", 25, \"Marketing\", 63000, \"Junior\")\n",
    "]\n",
    "\n",
    "# Define schema columns\n",
    "columns = [\"Name\", \"Age\", \"Department\", \"Salary\", \"Level\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(employee_data, columns)\n",
    "\n",
    "# Cache the DataFrame for performance\n",
    "df.cache()\n",
    "\n",
    "print(f\"âœ… Dataset Created: {df.count()} employees\")\n",
    "print(\"ğŸ“‹ Sample Data:\")\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a691a01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¢ Analyzing Department Statistics...\n",
      "ğŸ“Š Department Statistics:\n",
      "+-----------+--------------+------------------+-----------------+----------+----------+\n",
      "| Department|Employee_Count|           Avg_Age|       Avg_Salary|Min_Salary|Max_Salary|\n",
      "+-----------+--------------+------------------+-----------------+----------+----------+\n",
      "|Engineering|             4|             28.25|          78000.0|     72000|     85000|\n",
      "|      Sales|             3|33.333333333333336|          68000.0|     65000|     71000|\n",
      "|  Marketing|             3|              28.0|67333.33333333333|     63000|     70000|\n",
      "+-----------+--------------+------------------+-----------------+----------+----------+\n",
      "\n",
      "ğŸ‘¥ Level Distribution by Department:\n",
      "+-----------+--------------+------------------+-----------------+----------+----------+\n",
      "| Department|Employee_Count|           Avg_Age|       Avg_Salary|Min_Salary|Max_Salary|\n",
      "+-----------+--------------+------------------+-----------------+----------+----------+\n",
      "|Engineering|             4|             28.25|          78000.0|     72000|     85000|\n",
      "|      Sales|             3|33.333333333333336|          68000.0|     65000|     71000|\n",
      "|  Marketing|             3|              28.0|67333.33333333333|     63000|     70000|\n",
      "+-----------+--------------+------------------+-----------------+----------+----------+\n",
      "\n",
      "ğŸ‘¥ Level Distribution by Department:\n",
      "+-----------+-------+-----+\n",
      "| Department|  Level|count|\n",
      "+-----------+-------+-----+\n",
      "|Engineering| Senior|    3|\n",
      "|Engineering| Junior|    1|\n",
      "|  Marketing| Junior|    2|\n",
      "|  Marketing| Senior|    1|\n",
      "|      Sales|Manager|    2|\n",
      "|      Sales| Senior|    1|\n",
      "+-----------+-------+-----+\n",
      "\n",
      "+-----------+-------+-----+\n",
      "| Department|  Level|count|\n",
      "+-----------+-------+-----+\n",
      "|Engineering| Senior|    3|\n",
      "|Engineering| Junior|    1|\n",
      "|  Marketing| Junior|    2|\n",
      "|  Marketing| Senior|    1|\n",
      "|      Sales|Manager|    2|\n",
      "|      Sales| Senior|    1|\n",
      "+-----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEPARTMENT ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ¢ Analyzing Department Statistics...\")\n",
    "\n",
    "# Group by department and calculate metrics\n",
    "dept_stats = df.groupBy(\"Department\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"Employee_Count\"),\n",
    "        F.avg(\"Age\").alias(\"Avg_Age\"),\n",
    "        F.avg(\"Salary\").alias(\"Avg_Salary\"),\n",
    "        F.min(\"Salary\").alias(\"Min_Salary\"),\n",
    "        F.max(\"Salary\").alias(\"Max_Salary\")\n",
    "    ) \\\n",
    "    .orderBy(F.desc(\"Employee_Count\"))\n",
    "\n",
    "print(\"ğŸ“Š Department Statistics:\")\n",
    "dept_stats.show()\n",
    "\n",
    "# Level distribution analysis\n",
    "level_stats = df.groupBy(\"Department\", \"Level\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"Department\", F.desc(\"count\"))\n",
    "\n",
    "print(\"ğŸ‘¥ Level Distribution by Department:\")\n",
    "level_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c011afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’° Performing Salary Analysis...\n",
      "ğŸŒŸ High Earners (Salary > $70,000):\n",
      "+-------------+---+-----------+------+-------+\n",
      "|         Name|Age| Department|Salary|  Level|\n",
      "+-------------+---+-----------+------+-------+\n",
      "|     Ivy Chen| 30|Engineering| 85000| Senior|\n",
      "|  David Brown| 29|Engineering| 80000| Senior|\n",
      "|Alice Johnson| 28|Engineering| 75000| Senior|\n",
      "| Frank Miller| 26|Engineering| 72000| Junior|\n",
      "| Henry Taylor| 34|      Sales| 71000|Manager|\n",
      "+-------------+---+-----------+------+-------+\n",
      "\n",
      "ğŸ“ˆ Salary vs Department Average:\n",
      "+-------------+-----------+------+-----------------+-------------+--------------------+\n",
      "|Name         |Department |Salary|Avg_Salary       |Salary_vs_Avg|Performance_Category|\n",
      "+-------------+-----------+------+-----------------+-------------+--------------------+\n",
      "|Ivy Chen     |Engineering|85000 |78000.0          |7000.0       |Above Average       |\n",
      "|Henry Taylor |Sales      |71000 |68000.0          |3000.0       |Average             |\n",
      "|Catherine Lee|Marketing  |70000 |67333.33333333333|2666.67      |Average             |\n",
      "|David Brown  |Engineering|80000 |78000.0          |2000.0       |Average             |\n",
      "|Grace Davis  |Marketing  |69000 |67333.33333333333|1666.67      |Average             |\n",
      "|Eva Wilson   |Sales      |68000 |68000.0          |0.0          |Average             |\n",
      "|Alice Johnson|Engineering|75000 |78000.0          |-3000.0      |Average             |\n",
      "|Bob Smith    |Sales      |65000 |68000.0          |-3000.0      |Average             |\n",
      "|Jack Anderson|Marketing  |63000 |67333.33333333333|-4333.33     |Average             |\n",
      "|Frank Miller |Engineering|72000 |78000.0          |-6000.0      |Below Average       |\n",
      "+-------------+-----------+------+-----------------+-------------+--------------------+\n",
      "\n",
      "+-------------+-----------+------+-----------------+-------------+--------------------+\n",
      "|Name         |Department |Salary|Avg_Salary       |Salary_vs_Avg|Performance_Category|\n",
      "+-------------+-----------+------+-----------------+-------------+--------------------+\n",
      "|Ivy Chen     |Engineering|85000 |78000.0          |7000.0       |Above Average       |\n",
      "|Henry Taylor |Sales      |71000 |68000.0          |3000.0       |Average             |\n",
      "|Catherine Lee|Marketing  |70000 |67333.33333333333|2666.67      |Average             |\n",
      "|David Brown  |Engineering|80000 |78000.0          |2000.0       |Average             |\n",
      "|Grace Davis  |Marketing  |69000 |67333.33333333333|1666.67      |Average             |\n",
      "|Eva Wilson   |Sales      |68000 |68000.0          |0.0          |Average             |\n",
      "|Alice Johnson|Engineering|75000 |78000.0          |-3000.0      |Average             |\n",
      "|Bob Smith    |Sales      |65000 |68000.0          |-3000.0      |Average             |\n",
      "|Jack Anderson|Marketing  |63000 |67333.33333333333|-4333.33     |Average             |\n",
      "|Frank Miller |Engineering|72000 |78000.0          |-6000.0      |Below Average       |\n",
      "+-------------+-----------+------+-----------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SALARY ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ’° Performing Salary Analysis...\")\n",
    "\n",
    "# High earners analysis\n",
    "high_earners = df.filter(F.col(\"Salary\") > 70000) \\\n",
    "    .orderBy(F.desc(\"Salary\"))\n",
    "\n",
    "print(\"ğŸŒŸ High Earners (Salary > $70,000):\")\n",
    "high_earners.show()\n",
    "\n",
    "# Salary comparison with department average\n",
    "dept_avg_salary = dept_stats.select(\"Department\", \"Avg_Salary\")\n",
    "\n",
    "# Join employee data with department averages\n",
    "salary_comparison = df.join(dept_avg_salary, \"Department\", \"left\") \\\n",
    "    .withColumn(\"Salary_vs_Avg\", F.round(F.col(\"Salary\") - F.col(\"Avg_Salary\"), 2)) \\\n",
    "    .withColumn(\"Performance_Category\", \n",
    "        F.when(F.col(\"Salary_vs_Avg\") > 5000, \"Above Average\")\n",
    "         .when(F.col(\"Salary_vs_Avg\") < -5000, \"Below Average\")\n",
    "         .otherwise(\"Average\")\n",
    "    ) \\\n",
    "    .select(\"Name\", \"Department\", \"Salary\", \"Avg_Salary\", \"Salary_vs_Avg\", \"Performance_Category\")\n",
    "\n",
    "print(\"ğŸ“ˆ Salary vs Department Average:\")\n",
    "salary_comparison.orderBy(F.desc(\"Salary_vs_Avg\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84bf8fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘¥ Analyzing Age Demographics...\n",
      "ğŸ‚ Age Group Distribution:\n",
      "+------------------+-----+----------+-------+\n",
      "|         Age_Group|Count|Avg_Salary|Avg_Age|\n",
      "+------------------+-----+----------+-------+\n",
      "|       Young (â‰¤27)|    3|   68000.0|   26.0|\n",
      "|Mid-Career (28-32)|    5|   75600.0|   30.0|\n",
      "| Experienced (33+)|    2|   68000.0|   34.5|\n",
      "+------------------+-----+----------+-------+\n",
      "\n",
      "ğŸ¢ Age Groups by Department:\n",
      "+------------------+-----+----------+-------+\n",
      "|         Age_Group|Count|Avg_Salary|Avg_Age|\n",
      "+------------------+-----+----------+-------+\n",
      "|       Young (â‰¤27)|    3|   68000.0|   26.0|\n",
      "|Mid-Career (28-32)|    5|   75600.0|   30.0|\n",
      "| Experienced (33+)|    2|   68000.0|   34.5|\n",
      "+------------------+-----+----------+-------+\n",
      "\n",
      "ğŸ¢ Age Groups by Department:\n",
      "+-----------+------------------+-----+\n",
      "| Department|         Age_Group|count|\n",
      "+-----------+------------------+-----+\n",
      "|Engineering|Mid-Career (28-32)|    3|\n",
      "|Engineering|       Young (â‰¤27)|    1|\n",
      "|  Marketing|Mid-Career (28-32)|    1|\n",
      "|  Marketing|       Young (â‰¤27)|    2|\n",
      "|      Sales| Experienced (33+)|    2|\n",
      "|      Sales|Mid-Career (28-32)|    1|\n",
      "+-----------+------------------+-----+\n",
      "\n",
      "+-----------+------------------+-----+\n",
      "| Department|         Age_Group|count|\n",
      "+-----------+------------------+-----+\n",
      "|Engineering|Mid-Career (28-32)|    3|\n",
      "|Engineering|       Young (â‰¤27)|    1|\n",
      "|  Marketing|Mid-Career (28-32)|    1|\n",
      "|  Marketing|       Young (â‰¤27)|    2|\n",
      "|      Sales| Experienced (33+)|    2|\n",
      "|      Sales|Mid-Career (28-32)|    1|\n",
      "+-----------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# AGE GROUP ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ‘¥ Analyzing Age Demographics...\")\n",
    "\n",
    "# Create age groups\n",
    "df_with_age_groups = df.withColumn(\"Age_Group\",\n",
    "    F.when(F.col(\"Age\") <= 27, \"Young (â‰¤27)\")\n",
    "     .when(F.col(\"Age\") <= 32, \"Mid-Career (28-32)\")\n",
    "     .otherwise(\"Experienced (33+)\")\n",
    ")\n",
    "\n",
    "# Age group distribution\n",
    "age_group_distribution = df_with_age_groups.groupBy(\"Age_Group\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"Count\"),\n",
    "        F.avg(\"Salary\").alias(\"Avg_Salary\"),\n",
    "        F.avg(\"Age\").alias(\"Avg_Age\")\n",
    "    ) \\\n",
    "    .orderBy(\"Avg_Age\")\n",
    "\n",
    "print(\"ğŸ‚ Age Group Distribution:\")\n",
    "age_group_distribution.show()\n",
    "\n",
    "# Cross-analysis: Age Group vs Department\n",
    "age_dept_analysis = df_with_age_groups.groupBy(\"Department\", \"Age_Group\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"Department\", \"Age_Group\")\n",
    "\n",
    "print(\"ğŸ¢ Age Groups by Department:\")\n",
    "age_dept_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8fc3b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Final Summary Report\n",
      "============================================================\n",
      "ğŸ‘¥ Total Employees: 10\n",
      "ğŸ¢ Total Departments: 3\n",
      "ğŸ’° Average Salary: $71,800.00\n",
      "ğŸ‚ Average Age: 29.7 years\n",
      "ğŸŒŸ High Earners (>$70K): 5\n",
      "\n",
      "âš¡ Performance Metrics:\n",
      "ğŸ“ Event Logging: âœ… Enabled\n",
      "ğŸ’¾ DataFrame Cached: âœ… Yes\n",
      "ğŸ”— Cluster Mode: âœ… Docker Cluster\n",
      "\n",
      "ğŸ—‘ï¸ Cache cleared\n",
      "============================================================\n",
      "â° Analysis completed at: 2025-09-03 21:42:48\n",
      "ğŸ‘¥ Total Employees: 10\n",
      "ğŸ¢ Total Departments: 3\n",
      "ğŸ’° Average Salary: $71,800.00\n",
      "ğŸ‚ Average Age: 29.7 years\n",
      "ğŸŒŸ High Earners (>$70K): 5\n",
      "\n",
      "âš¡ Performance Metrics:\n",
      "ğŸ“ Event Logging: âœ… Enabled\n",
      "ğŸ’¾ DataFrame Cached: âœ… Yes\n",
      "ğŸ”— Cluster Mode: âœ… Docker Cluster\n",
      "\n",
      "ğŸ—‘ï¸ Cache cleared\n",
      "============================================================\n",
      "â° Analysis completed at: 2025-09-03 21:42:48\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY & CLEANUP\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ“Š Final Summary Report\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate key metrics\n",
    "total_employees = df.count()\n",
    "total_departments = df.select(\"Department\").distinct().count()\n",
    "avg_salary = df.agg(F.avg(\"Salary\")).collect()[0][0]\n",
    "avg_age = df.agg(F.avg(\"Age\")).collect()[0][0]\n",
    "high_earner_count = high_earners.count()\n",
    "\n",
    "print(f\"ğŸ‘¥ Total Employees: {total_employees}\")\n",
    "print(f\"ğŸ¢ Total Departments: {total_departments}\")\n",
    "print(f\"ğŸ’° Average Salary: ${avg_salary:,.2f}\")\n",
    "print(f\"ğŸ‚ Average Age: {avg_age:.1f} years\")\n",
    "print(f\"ğŸŒŸ High Earners (>$70K): {high_earner_count}\")\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"\\nâš¡ Performance Metrics:\")\n",
    "print(f\"ğŸ“ Event Logging: {'âœ… Enabled' if spark.sparkContext.getConf().get('spark.eventLog.enabled') == 'true' else 'âŒ Disabled'}\")\n",
    "print(f\"ğŸ’¾ DataFrame Cached: {'âœ… Yes' if df.is_cached else 'âŒ No'}\")\n",
    "print(f\"ğŸ”— Cluster Mode: {'âœ… Docker Cluster' if 'spark://' in spark.sparkContext.master else 'âŒ Local Mode'}\")\n",
    "\n",
    "# Unpersist cached DataFrame\n",
    "df.unpersist()\n",
    "print(f\"\\nğŸ—‘ï¸ Cache cleared\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"â° Analysis completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "678e3063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›‘ Terminating Spark Session...\n",
      "âœ… Spark session terminated successfully\n",
      "ğŸ“‹ Application: DockerClusterDemo\n",
      "ğŸ†” Application ID: app-20250903144158-0016\n",
      "\n",
      "ğŸ“ Event Log Files (5 total):\n",
      "   ğŸ“„ app-20250903143443-0014.zstd (223,506 bytes)\n",
      "   ğŸ“„ app-20250903143812-0015.zstd (224,462 bytes)\n",
      "   ğŸ“„ app-20250903144158-0016.zstd (278,983 bytes)\n",
      "   ğŸ“„ local-1756909626239 (189,404 bytes)\n",
      "   ğŸ“„ local-1756909983686 (1,001,424 bytes)\n",
      "\n",
      "ğŸ” Monitor URLs (may require application restart):\n",
      "   ğŸ“Š Cluster Master: http://localhost:8080\n",
      "   ğŸ“ˆ History Server: http://localhost:18080\n",
      "   ğŸ³ JupyterLab: http://localhost:8888\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ Docker Cluster Demo Completed Successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SESSION TERMINATION & EVENT LOG VERIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ›‘ Terminating Spark Session...\")\n",
    "\n",
    "# Get session info before stopping\n",
    "app_id = spark.sparkContext.applicationId\n",
    "app_name = spark.sparkContext.appName\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "print(f\"âœ… Spark session terminated successfully\")\n",
    "print(f\"ğŸ“‹ Application: {app_name}\")\n",
    "print(f\"ğŸ†” Application ID: {app_id}\")\n",
    "\n",
    "# Verify event logs were created\n",
    "import glob\n",
    "event_files = glob.glob(os.path.join(events_dir, \"*\"))\n",
    "print(f\"\\nğŸ“ Event Log Files ({len(event_files)} total):\")\n",
    "\n",
    "for event_file in sorted(event_files):\n",
    "    file_name = os.path.basename(event_file)\n",
    "    file_size = os.path.getsize(event_file)\n",
    "    print(f\"   ğŸ“„ {file_name} ({file_size:,} bytes)\")\n",
    "\n",
    "print(f\"\\nğŸ” Monitor URLs (may require application restart):\")\n",
    "print(f\"   ğŸ“Š Cluster Master: http://localhost:8080\")\n",
    "print(f\"   ğŸ“ˆ History Server: http://localhost:18080\")\n",
    "print(f\"   ğŸ³ JupyterLab: http://localhost:8888\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ Docker Cluster Demo Completed Successfully!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b27904",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ What We Accomplished\n",
    "\n",
    "This notebook successfully demonstrated:\n",
    "\n",
    "### âœ… **Technical Achievements**\n",
    "- **Docker Cluster Integration**: External driver connecting to containerized Spark cluster\n",
    "- **Event Logging**: Comprehensive tracking with compressed storage\n",
    "- **Advanced Analytics**: Multi-dimensional data analysis with joins and aggregations\n",
    "- **Performance Optimization**: DataFrame caching and adaptive query execution\n",
    "\n",
    "### ğŸ“Š **Analytics Results**\n",
    "- **Employee Dataset**: 10 records across 3 departments\n",
    "- **Department Analysis**: Engineering, Sales, Marketing statistics\n",
    "- **Salary Analysis**: Performance categorization and comparisons\n",
    "- **Demographics**: Age group distributions and cross-analysis\n",
    "\n",
    "### ğŸ”§ **Infrastructure Components**\n",
    "- **Spark Master**: Cluster coordination and resource management\n",
    "- **Spark Workers**: Distributed computation execution\n",
    "- **History Server**: Event log analysis and visualization\n",
    "- **Event Storage**: Persistent logging with compression\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Next Steps\n",
    "\n",
    "1. **Scale the Dataset**: Replace sample data with real datasets\n",
    "2. **Advanced Analytics**: Implement machine learning pipelines\n",
    "3. **Performance Tuning**: Optimize cluster resources for larger workloads\n",
    "4. **Monitoring**: Set up alerting and metrics collection\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Resources\n",
    "\n",
    "- **Apache Spark Documentation**: https://spark.apache.org/docs/latest/\n",
    "- **Docker Compose Reference**: https://docs.docker.com/compose/\n",
    "- **PySpark API**: https://spark.apache.org/docs/latest/api/python/\n",
    "\n",
    "---\n",
    "\n",
    "*Demo completed successfully! All events have been logged and are available in the History Server.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
