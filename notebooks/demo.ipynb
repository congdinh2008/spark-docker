{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3ef27d",
   "metadata": {},
   "source": [
    "# Spark Docker Demo - Dual Environment Support ✨\n",
    "\n",
    "**✅ FIXED**: This notebook now works in both VS Code and JupyterLab container!\n",
    "\n",
    "## 🎯 Auto Environment Detection\n",
    "\n",
    "This notebook automatically detects and configures for:\n",
    "\n",
    "### 💻 **VS Code Local Mode**\n",
    "- **Spark**: Local instance (`local[*]`)\n",
    "- **Events**: Saved to local directory \n",
    "- **UI**: http://localhost:4040\n",
    "- **Perfect for**: Development and testing\n",
    "\n",
    "### 🐳 **Docker Container Mode** \n",
    "- **Spark**: Cluster connection (`spark://spark-master:7077`)\n",
    "- **Events**: Shared with History Server\n",
    "- **UI**: Full cluster monitoring\n",
    "- **Perfect for**: Production-like environment\n",
    "\n",
    "## 🚀 How to Use:\n",
    "\n",
    "### Option 1: VS Code (Current)\n",
    "1. Run cells directly in VS Code\n",
    "2. Events saved to `/Users/.../spark-docker/events`\n",
    "3. View at: http://localhost:4040\n",
    "\n",
    "### Option 2: JupyterLab Container\n",
    "1. Make sure containers are running: `docker compose up -d`\n",
    "2. Open browser: **http://localhost:8888**\n",
    "3. Navigate to this notebook and run cells\n",
    "4. View cluster at: http://localhost:8080\n",
    "\n",
    "## 🔧 Current Setup:\n",
    "- **Spark Version**: 3.5.0 (Optimized)\n",
    "- **Auto Detection**: ✅ Working\n",
    "- **Event Logging**: ✅ Enabled for both environments\n",
    "- **Health Checks**: ✅ All services healthy\n",
    "- **Dependencies**: ✅ Perfect startup sequence\n",
    "\n",
    "## 📊 Monitoring URLs:\n",
    "- **Local Spark UI**: http://localhost:4040 (VS Code)\n",
    "- **Master UI**: http://localhost:8080 (Docker cluster)\n",
    "- **History Server**: http://localhost:18080 (All events)\n",
    "- **JupyterLab**: http://localhost:8888 (Container access)\n",
    "\n",
    "## ✨ New Features:\n",
    "- ✅ **Dual Environment Support**: Works in VS Code + Docker\n",
    "- ✅ **Auto Configuration**: Detects environment automatically  \n",
    "- ✅ **Event Generation**: Creates events in both modes\n",
    "- ✅ **Performance Testing**: Caching and optimization demos\n",
    "- ✅ **Comprehensive Monitoring**: Full cluster information\n",
    "\n",
    "## 🐛 Troubleshooting:\n",
    "- **VS Code**: Events saved locally, check `/events` directory\n",
    "- **Container**: If connection fails, restart: `docker compose restart`\n",
    "- **UI Access**: All UIs should be accessible simultaneously\n",
    "- **Events**: History Server shows events from both environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import os\n",
    "import socket\n",
    "\n",
    "# Detect environment và cấu hình phù hợp\n",
    "def detect_environment():\n",
    "    \"\"\"Detect if running in Docker container or local VS Code\"\"\"\n",
    "    try:\n",
    "        # Check if we're in JupyterLab container\n",
    "        hostname = socket.gethostname()\n",
    "        if \"jupyter\" in hostname or os.path.exists(\"/home/jovyan\"):\n",
    "            return \"jupyter_container\"\n",
    "        # Check if we can connect to Spark cluster\n",
    "        elif os.path.exists(\"/Users/congdinh/Downloads/work/content/de/spark-docker/events\"):\n",
    "            return \"vscode_local\"\n",
    "        else:\n",
    "            return \"vscode_local\"\n",
    "    except:\n",
    "        return \"vscode_local\"\n",
    "\n",
    "environment = detect_environment()\n",
    "print(f\"🔍 Detected environment: {environment}\")\n",
    "\n",
    "# Cấu hình dựa trên environment\n",
    "if environment == \"jupyter_container\":\n",
    "    print(\"🐳 Running in JupyterLab container - using cluster configuration\")\n",
    "    spark_config = SparkSession.builder \\\n",
    "        .appName(\"OptimizedSparkDemo\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "        .config(\"spark.driver.host\", \"jupyter-lab\") \\\n",
    "        .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "        .config(\"spark.eventLog.dir\", \"file:///events\")\n",
    "else:\n",
    "    print(\"💻 Running in VS Code - using local configuration\")\n",
    "    # Tạo local events directory nếu chưa có\n",
    "    local_events_dir = \"/Users/congdinh/Downloads/work/content/de/spark-docker/events\"\n",
    "    os.makedirs(local_events_dir, exist_ok=True)\n",
    "    \n",
    "    spark_config = SparkSession.builder \\\n",
    "        .appName(\"LocalSparkDemo\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "        .config(\"spark.eventLog.dir\", f\"file://{local_events_dir}\") \\\n",
    "        .config(\"spark.ui.enabled\", \"true\") \\\n",
    "        .config(\"spark.ui.port\", \"4040\")\n",
    "\n",
    "# Tạo SparkSession\n",
    "spark = spark_config.getOrCreate()\n",
    "\n",
    "print(f\"✅ Spark Version: {spark.version}\")\n",
    "print(f\"🎯 Spark Master URL: {spark.conf.get('spark.master')}\")\n",
    "print(f\"📱 Spark App Name: {spark.conf.get('spark.app.name')}\")\n",
    "print(f\"📝 Event Log Dir: {spark.conf.get('spark.eventLog.dir')}\")\n",
    "\n",
    "# Test với data đơn giản\n",
    "print(\"\\n=== Basic DataFrame Test ===\")\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# Test performance với data lớn hơn\n",
    "print(\"\\n=== Performance Test ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Tạo DataFrame lớn hơn để test\n",
    "large_data = [(f\"User_{i}\", i % 100) for i in range(10000)]\n",
    "large_df = spark.createDataFrame(large_data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Thực hiện một số operations\n",
    "result = large_df.groupBy(\"Age\").count().orderBy(\"Age\")\n",
    "result.show(10)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"⏱️ Processing time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\n=== Spark UI URLs ===\")\n",
    "print(f\"🌐 Spark Application UI: {spark.sparkContext.uiWebUrl}\")\n",
    "if environment == \"jupyter_container\":\n",
    "    print(\"🔗 Master UI: http://localhost:8080\")\n",
    "    print(\"📊 History Server: http://localhost:18080\")\n",
    "else:\n",
    "    print(\"🔗 Local Spark UI: http://localhost:4040\")\n",
    "    print(\"📁 Events Directory: /Users/congdinh/Downloads/work/content/de/spark-docker/events\")\n",
    "\n",
    "print(f\"\\n✨ Environment: {environment}\")\n",
    "print(\"🚀 SparkSession is ready for use!\")\n",
    "\n",
    "# Không stop context để có thể xem UI và events\n",
    "# spark.stop()  # Comment out để giữ SparkSession active"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
