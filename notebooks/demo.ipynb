{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3ef27d",
   "metadata": {},
   "source": [
    "# Apache Spark Docker Cluster Demo\n",
    "\n",
    "**Professional demonstration of Apache Spark running on Docker with comprehensive event logging**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Overview\n",
    "\n",
    "This notebook demonstrates a complete Apache Spark setup with:\n",
    "\n",
    "- **Docker Cluster Mode**: Spark master and workers running in containers\n",
    "- **Event Logging**: Comprehensive tracking of all Spark operations\n",
    "- **Advanced Analytics**: Real-world data processing examples\n",
    "- **Monitoring Integration**: Web UIs for cluster and application monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Architecture\n",
    "\n",
    "### Cluster Components\n",
    "- **Spark Master**: Cluster coordinator (port 7077, UI port 8080)\n",
    "- **Spark Workers**: Execution nodes \n",
    "- **History Server**: Event log analysis (port 18080)\n",
    "- **JupyterLab**: Alternative notebook interface (port 8888)\n",
    "\n",
    "### Driver Configuration\n",
    "- **External Driver**: VS Code notebook connects to Docker cluster\n",
    "- **Event Storage**: Local filesystem shared with containers\n",
    "- **Network**: `host.docker.internal` for cross-container communication\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Monitoring Dashboard URLs\n",
    "\n",
    "| Service | URL | Description |\n",
    "|---------|-----|-------------|\n",
    "| Cluster Master | http://localhost:8080 | Live cluster status |\n",
    "| Application UI | http://host.docker.internal:4040 | Current job details |\n",
    "| History Server | http://localhost:18080 | Historical event logs |\n",
    "| JupyterLab | http://localhost:8888 | Container notebook interface |\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "1. **Start containers**: `docker compose up -d`\n",
    "2. **Run cells below**: Execute the Spark configuration and analytics\n",
    "3. **Monitor execution**: Use the dashboard URLs above\n",
    "4. **View event logs**: Check History Server for detailed execution logs\n",
    "\n",
    "---\n",
    "\n",
    "## \udcdd Technical Specifications\n",
    "\n",
    "- **Spark Version**: 3.5.0\n",
    "- **Event Compression**: zstd algorithm\n",
    "- **Serialization**: Kryo for performance\n",
    "- **Memory**: 1GB driver + 1GB executor\n",
    "- **Adaptive Query Execution**: Enabled\n",
    "- **Event Directory**: `./events/` (shared volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e6e58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê≥ Initializing Spark Docker Cluster Connection\n",
      "============================================================\n",
      "üìÅ Events Directory: /Users/congdinh/Downloads/work/content/de/spark-docker/events\n",
      "‚è∞ Session Start: 2025-09-03 21:41:51\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SPARK CLUSTER CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"üê≥ Initializing Spark Docker Cluster Connection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "current_dir = os.getcwd()\n",
    "workspace_root = os.path.dirname(current_dir)\n",
    "events_dir = os.path.join(workspace_root, \"events\")\n",
    "events_uri = f\"file://{events_dir}\"\n",
    "\n",
    "# Ensure events directory exists\n",
    "os.makedirs(events_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Events Directory: {events_dir}\")\n",
    "print(f\"‚è∞ Session Start: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d1b3d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating Spark Session...\n",
      "‚úÖ Spark Session Created Successfully!\n",
      "üìã Application ID: app-20250903144158-0016\n",
      "üéØ Master: spark://localhost:7077\n",
      "üåê Application UI: http://host.docker.internal:4040\n",
      "üìù Event Logging: true\n",
      "üìÅ Event Directory: file:///Users/congdinh/Downloads/work/content/de/spark-docker/events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/03 21:41:58 WARN Utils: Service 'sparkDriver' could not bind on port 7078. Attempting port 7079.\n",
      "25/09/03 21:41:58 WARN Utils: Service 'sparkDriver' could not bind on port 7079. Attempting port 7080.\n",
      "25/09/03 21:41:58 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 7079. Attempting port 7080.\n",
      "25/09/03 21:41:58 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 7080. Attempting port 7081.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/03 21:42:14 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SPARK SESSION CREATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ Creating Spark Session...\")\n",
    "\n",
    "# Build Spark session with optimized configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DockerClusterDemo\") \\\n",
    "    .master(\"spark://localhost:7077\") \\\n",
    "    .config(\"spark.driver.host\", \"host.docker.internal\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.driver.port\", \"7078\") \\\n",
    "    .config(\"spark.blockManager.port\", \"7079\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "    .config(\"spark.eventLog.dir\", events_uri) \\\n",
    "    .config(\"spark.eventLog.compress\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Display session information\n",
    "print(\"‚úÖ Spark Session Created Successfully!\")\n",
    "print(f\"üìã Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"üéØ Master: {spark.sparkContext.master}\")\n",
    "print(f\"üåê Application UI: {spark.sparkContext.uiWebUrl}\")\n",
    "print(f\"üìù Event Logging: {spark.sparkContext.getConf().get('spark.eventLog.enabled')}\")\n",
    "print(f\"üìÅ Event Directory: {events_uri}\")\n",
    "\n",
    "# Set log level to reduce noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee52d7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating Sample Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset Created: 10 employees\n",
      "üìã Sample Data:\n",
      "+-------------+---+-----------+------+-------+\n",
      "|Name         |Age|Department |Salary|Level  |\n",
      "+-------------+---+-----------+------+-------+\n",
      "|Alice Johnson|28 |Engineering|75000 |Senior |\n",
      "|Bob Smith    |35 |Sales      |65000 |Manager|\n",
      "|Catherine Lee|32 |Marketing  |70000 |Senior |\n",
      "|David Brown  |29 |Engineering|80000 |Senior |\n",
      "|Eva Wilson   |31 |Sales      |68000 |Senior |\n",
      "|Frank Miller |26 |Engineering|72000 |Junior |\n",
      "|Grace Davis  |27 |Marketing  |69000 |Junior |\n",
      "|Henry Taylor |34 |Sales      |71000 |Manager|\n",
      "|Ivy Chen     |30 |Engineering|85000 |Senior |\n",
      "|Jack Anderson|25 |Marketing  |63000 |Junior |\n",
      "+-------------+---+-----------+------+-------+\n",
      "\n",
      "+-------------+---+-----------+------+-------+\n",
      "|Name         |Age|Department |Salary|Level  |\n",
      "+-------------+---+-----------+------+-------+\n",
      "|Alice Johnson|28 |Engineering|75000 |Senior |\n",
      "|Bob Smith    |35 |Sales      |65000 |Manager|\n",
      "|Catherine Lee|32 |Marketing  |70000 |Senior |\n",
      "|David Brown  |29 |Engineering|80000 |Senior |\n",
      "|Eva Wilson   |31 |Sales      |68000 |Senior |\n",
      "|Frank Miller |26 |Engineering|72000 |Junior |\n",
      "|Grace Davis  |27 |Marketing  |69000 |Junior |\n",
      "|Henry Taylor |34 |Sales      |71000 |Manager|\n",
      "|Ivy Chen     |30 |Engineering|85000 |Senior |\n",
      "|Jack Anderson|25 |Marketing  |63000 |Junior |\n",
      "+-------------+---+-----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Creating Sample Dataset...\")\n",
    "\n",
    "# Sample employee data\n",
    "employee_data = [\n",
    "    (\"Alice Johnson\", 28, \"Engineering\", 75000, \"Senior\"),\n",
    "    (\"Bob Smith\", 35, \"Sales\", 65000, \"Manager\"),\n",
    "    (\"Catherine Lee\", 32, \"Marketing\", 70000, \"Senior\"),\n",
    "    (\"David Brown\", 29, \"Engineering\", 80000, \"Senior\"),\n",
    "    (\"Eva Wilson\", 31, \"Sales\", 68000, \"Senior\"),\n",
    "    (\"Frank Miller\", 26, \"Engineering\", 72000, \"Junior\"),\n",
    "    (\"Grace Davis\", 27, \"Marketing\", 69000, \"Junior\"),\n",
    "    (\"Henry Taylor\", 34, \"Sales\", 71000, \"Manager\"),\n",
    "    (\"Ivy Chen\", 30, \"Engineering\", 85000, \"Senior\"),\n",
    "    (\"Jack Anderson\", 25, \"Marketing\", 63000, \"Junior\")\n",
    "]\n",
    "\n",
    "# Define schema columns\n",
    "columns = [\"Name\", \"Age\", \"Department\", \"Salary\", \"Level\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(employee_data, columns)\n",
    "\n",
    "# Cache the DataFrame for performance\n",
    "df.cache()\n",
    "\n",
    "print(f\"‚úÖ Dataset Created: {df.count()} employees\")\n",
    "print(\"üìã Sample Data:\")\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a691a01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè¢ Analyzing Department Statistics...\n",
      "üìä Department Statistics:\n",
      "+-----------+--------------+------------------+-----------------+----------+----------+\n",
      "| Department|Employee_Count|           Avg_Age|       Avg_Salary|Min_Salary|Max_Salary|\n",
      "+-----------+--------------+------------------+-----------------+----------+----------+\n",
      "|Engineering|             4|             28.25|          78000.0|     72000|     85000|\n",
      "|      Sales|             3|33.333333333333336|          68000.0|     65000|     71000|\n",
      "|  Marketing|             3|              28.0|67333.33333333333|     63000|     70000|\n",
      "+-----------+--------------+------------------+-----------------+----------+----------+\n",
      "\n",
      "üë• Level Distribution by Department:\n",
      "+-----------+--------------+------------------+-----------------+----------+----------+\n",
      "| Department|Employee_Count|           Avg_Age|       Avg_Salary|Min_Salary|Max_Salary|\n",
      "+-----------+--------------+------------------+-----------------+----------+----------+\n",
      "|Engineering|             4|             28.25|          78000.0|     72000|     85000|\n",
      "|      Sales|             3|33.333333333333336|          68000.0|     65000|     71000|\n",
      "|  Marketing|             3|              28.0|67333.33333333333|     63000|     70000|\n",
      "+-----------+--------------+------------------+-----------------+----------+----------+\n",
      "\n",
      "üë• Level Distribution by Department:\n",
      "+-----------+-------+-----+\n",
      "| Department|  Level|count|\n",
      "+-----------+-------+-----+\n",
      "|Engineering| Senior|    3|\n",
      "|Engineering| Junior|    1|\n",
      "|  Marketing| Junior|    2|\n",
      "|  Marketing| Senior|    1|\n",
      "|      Sales|Manager|    2|\n",
      "|      Sales| Senior|    1|\n",
      "+-----------+-------+-----+\n",
      "\n",
      "+-----------+-------+-----+\n",
      "| Department|  Level|count|\n",
      "+-----------+-------+-----+\n",
      "|Engineering| Senior|    3|\n",
      "|Engineering| Junior|    1|\n",
      "|  Marketing| Junior|    2|\n",
      "|  Marketing| Senior|    1|\n",
      "|      Sales|Manager|    2|\n",
      "|      Sales| Senior|    1|\n",
      "+-----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEPARTMENT ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üè¢ Analyzing Department Statistics...\")\n",
    "\n",
    "# Group by department and calculate metrics\n",
    "dept_stats = df.groupBy(\"Department\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"Employee_Count\"),\n",
    "        F.avg(\"Age\").alias(\"Avg_Age\"),\n",
    "        F.avg(\"Salary\").alias(\"Avg_Salary\"),\n",
    "        F.min(\"Salary\").alias(\"Min_Salary\"),\n",
    "        F.max(\"Salary\").alias(\"Max_Salary\")\n",
    "    ) \\\n",
    "    .orderBy(F.desc(\"Employee_Count\"))\n",
    "\n",
    "print(\"üìä Department Statistics:\")\n",
    "dept_stats.show()\n",
    "\n",
    "# Level distribution analysis\n",
    "level_stats = df.groupBy(\"Department\", \"Level\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"Department\", F.desc(\"count\"))\n",
    "\n",
    "print(\"üë• Level Distribution by Department:\")\n",
    "level_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c011afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ Performing Salary Analysis...\n",
      "üåü High Earners (Salary > $70,000):\n",
      "+-------------+---+-----------+------+-------+\n",
      "|         Name|Age| Department|Salary|  Level|\n",
      "+-------------+---+-----------+------+-------+\n",
      "|     Ivy Chen| 30|Engineering| 85000| Senior|\n",
      "|  David Brown| 29|Engineering| 80000| Senior|\n",
      "|Alice Johnson| 28|Engineering| 75000| Senior|\n",
      "| Frank Miller| 26|Engineering| 72000| Junior|\n",
      "| Henry Taylor| 34|      Sales| 71000|Manager|\n",
      "+-------------+---+-----------+------+-------+\n",
      "\n",
      "üìà Salary vs Department Average:\n",
      "+-------------+-----------+------+-----------------+-------------+--------------------+\n",
      "|Name         |Department |Salary|Avg_Salary       |Salary_vs_Avg|Performance_Category|\n",
      "+-------------+-----------+------+-----------------+-------------+--------------------+\n",
      "|Ivy Chen     |Engineering|85000 |78000.0          |7000.0       |Above Average       |\n",
      "|Henry Taylor |Sales      |71000 |68000.0          |3000.0       |Average             |\n",
      "|Catherine Lee|Marketing  |70000 |67333.33333333333|2666.67      |Average             |\n",
      "|David Brown  |Engineering|80000 |78000.0          |2000.0       |Average             |\n",
      "|Grace Davis  |Marketing  |69000 |67333.33333333333|1666.67      |Average             |\n",
      "|Eva Wilson   |Sales      |68000 |68000.0          |0.0          |Average             |\n",
      "|Alice Johnson|Engineering|75000 |78000.0          |-3000.0      |Average             |\n",
      "|Bob Smith    |Sales      |65000 |68000.0          |-3000.0      |Average             |\n",
      "|Jack Anderson|Marketing  |63000 |67333.33333333333|-4333.33     |Average             |\n",
      "|Frank Miller |Engineering|72000 |78000.0          |-6000.0      |Below Average       |\n",
      "+-------------+-----------+------+-----------------+-------------+--------------------+\n",
      "\n",
      "+-------------+-----------+------+-----------------+-------------+--------------------+\n",
      "|Name         |Department |Salary|Avg_Salary       |Salary_vs_Avg|Performance_Category|\n",
      "+-------------+-----------+------+-----------------+-------------+--------------------+\n",
      "|Ivy Chen     |Engineering|85000 |78000.0          |7000.0       |Above Average       |\n",
      "|Henry Taylor |Sales      |71000 |68000.0          |3000.0       |Average             |\n",
      "|Catherine Lee|Marketing  |70000 |67333.33333333333|2666.67      |Average             |\n",
      "|David Brown  |Engineering|80000 |78000.0          |2000.0       |Average             |\n",
      "|Grace Davis  |Marketing  |69000 |67333.33333333333|1666.67      |Average             |\n",
      "|Eva Wilson   |Sales      |68000 |68000.0          |0.0          |Average             |\n",
      "|Alice Johnson|Engineering|75000 |78000.0          |-3000.0      |Average             |\n",
      "|Bob Smith    |Sales      |65000 |68000.0          |-3000.0      |Average             |\n",
      "|Jack Anderson|Marketing  |63000 |67333.33333333333|-4333.33     |Average             |\n",
      "|Frank Miller |Engineering|72000 |78000.0          |-6000.0      |Below Average       |\n",
      "+-------------+-----------+------+-----------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SALARY ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üí∞ Performing Salary Analysis...\")\n",
    "\n",
    "# High earners analysis\n",
    "high_earners = df.filter(F.col(\"Salary\") > 70000) \\\n",
    "    .orderBy(F.desc(\"Salary\"))\n",
    "\n",
    "print(\"üåü High Earners (Salary > $70,000):\")\n",
    "high_earners.show()\n",
    "\n",
    "# Salary comparison with department average\n",
    "dept_avg_salary = dept_stats.select(\"Department\", \"Avg_Salary\")\n",
    "\n",
    "# Join employee data with department averages\n",
    "salary_comparison = df.join(dept_avg_salary, \"Department\", \"left\") \\\n",
    "    .withColumn(\"Salary_vs_Avg\", F.round(F.col(\"Salary\") - F.col(\"Avg_Salary\"), 2)) \\\n",
    "    .withColumn(\"Performance_Category\", \n",
    "        F.when(F.col(\"Salary_vs_Avg\") > 5000, \"Above Average\")\n",
    "         .when(F.col(\"Salary_vs_Avg\") < -5000, \"Below Average\")\n",
    "         .otherwise(\"Average\")\n",
    "    ) \\\n",
    "    .select(\"Name\", \"Department\", \"Salary\", \"Avg_Salary\", \"Salary_vs_Avg\", \"Performance_Category\")\n",
    "\n",
    "print(\"üìà Salary vs Department Average:\")\n",
    "salary_comparison.orderBy(F.desc(\"Salary_vs_Avg\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84bf8fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë• Analyzing Age Demographics...\n",
      "üéÇ Age Group Distribution:\n",
      "+------------------+-----+----------+-------+\n",
      "|         Age_Group|Count|Avg_Salary|Avg_Age|\n",
      "+------------------+-----+----------+-------+\n",
      "|       Young (‚â§27)|    3|   68000.0|   26.0|\n",
      "|Mid-Career (28-32)|    5|   75600.0|   30.0|\n",
      "| Experienced (33+)|    2|   68000.0|   34.5|\n",
      "+------------------+-----+----------+-------+\n",
      "\n",
      "üè¢ Age Groups by Department:\n",
      "+------------------+-----+----------+-------+\n",
      "|         Age_Group|Count|Avg_Salary|Avg_Age|\n",
      "+------------------+-----+----------+-------+\n",
      "|       Young (‚â§27)|    3|   68000.0|   26.0|\n",
      "|Mid-Career (28-32)|    5|   75600.0|   30.0|\n",
      "| Experienced (33+)|    2|   68000.0|   34.5|\n",
      "+------------------+-----+----------+-------+\n",
      "\n",
      "üè¢ Age Groups by Department:\n",
      "+-----------+------------------+-----+\n",
      "| Department|         Age_Group|count|\n",
      "+-----------+------------------+-----+\n",
      "|Engineering|Mid-Career (28-32)|    3|\n",
      "|Engineering|       Young (‚â§27)|    1|\n",
      "|  Marketing|Mid-Career (28-32)|    1|\n",
      "|  Marketing|       Young (‚â§27)|    2|\n",
      "|      Sales| Experienced (33+)|    2|\n",
      "|      Sales|Mid-Career (28-32)|    1|\n",
      "+-----------+------------------+-----+\n",
      "\n",
      "+-----------+------------------+-----+\n",
      "| Department|         Age_Group|count|\n",
      "+-----------+------------------+-----+\n",
      "|Engineering|Mid-Career (28-32)|    3|\n",
      "|Engineering|       Young (‚â§27)|    1|\n",
      "|  Marketing|Mid-Career (28-32)|    1|\n",
      "|  Marketing|       Young (‚â§27)|    2|\n",
      "|      Sales| Experienced (33+)|    2|\n",
      "|      Sales|Mid-Career (28-32)|    1|\n",
      "+-----------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# AGE GROUP ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üë• Analyzing Age Demographics...\")\n",
    "\n",
    "# Create age groups\n",
    "df_with_age_groups = df.withColumn(\"Age_Group\",\n",
    "    F.when(F.col(\"Age\") <= 27, \"Young (‚â§27)\")\n",
    "     .when(F.col(\"Age\") <= 32, \"Mid-Career (28-32)\")\n",
    "     .otherwise(\"Experienced (33+)\")\n",
    ")\n",
    "\n",
    "# Age group distribution\n",
    "age_group_distribution = df_with_age_groups.groupBy(\"Age_Group\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"Count\"),\n",
    "        F.avg(\"Salary\").alias(\"Avg_Salary\"),\n",
    "        F.avg(\"Age\").alias(\"Avg_Age\")\n",
    "    ) \\\n",
    "    .orderBy(\"Avg_Age\")\n",
    "\n",
    "print(\"üéÇ Age Group Distribution:\")\n",
    "age_group_distribution.show()\n",
    "\n",
    "# Cross-analysis: Age Group vs Department\n",
    "age_dept_analysis = df_with_age_groups.groupBy(\"Department\", \"Age_Group\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"Department\", \"Age_Group\")\n",
    "\n",
    "print(\"üè¢ Age Groups by Department:\")\n",
    "age_dept_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8fc3b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Final Summary Report\n",
      "============================================================\n",
      "üë• Total Employees: 10\n",
      "üè¢ Total Departments: 3\n",
      "üí∞ Average Salary: $71,800.00\n",
      "üéÇ Average Age: 29.7 years\n",
      "üåü High Earners (>$70K): 5\n",
      "\n",
      "‚ö° Performance Metrics:\n",
      "üìù Event Logging: ‚úÖ Enabled\n",
      "üíæ DataFrame Cached: ‚úÖ Yes\n",
      "üîó Cluster Mode: ‚úÖ Docker Cluster\n",
      "\n",
      "üóëÔ∏è Cache cleared\n",
      "============================================================\n",
      "‚è∞ Analysis completed at: 2025-09-03 21:42:48\n",
      "üë• Total Employees: 10\n",
      "üè¢ Total Departments: 3\n",
      "üí∞ Average Salary: $71,800.00\n",
      "üéÇ Average Age: 29.7 years\n",
      "üåü High Earners (>$70K): 5\n",
      "\n",
      "‚ö° Performance Metrics:\n",
      "üìù Event Logging: ‚úÖ Enabled\n",
      "üíæ DataFrame Cached: ‚úÖ Yes\n",
      "üîó Cluster Mode: ‚úÖ Docker Cluster\n",
      "\n",
      "üóëÔ∏è Cache cleared\n",
      "============================================================\n",
      "‚è∞ Analysis completed at: 2025-09-03 21:42:48\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY & CLEANUP\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Final Summary Report\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate key metrics\n",
    "total_employees = df.count()\n",
    "total_departments = df.select(\"Department\").distinct().count()\n",
    "avg_salary = df.agg(F.avg(\"Salary\")).collect()[0][0]\n",
    "avg_age = df.agg(F.avg(\"Age\")).collect()[0][0]\n",
    "high_earner_count = high_earners.count()\n",
    "\n",
    "print(f\"üë• Total Employees: {total_employees}\")\n",
    "print(f\"üè¢ Total Departments: {total_departments}\")\n",
    "print(f\"üí∞ Average Salary: ${avg_salary:,.2f}\")\n",
    "print(f\"üéÇ Average Age: {avg_age:.1f} years\")\n",
    "print(f\"üåü High Earners (>$70K): {high_earner_count}\")\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"\\n‚ö° Performance Metrics:\")\n",
    "print(f\"üìù Event Logging: {'‚úÖ Enabled' if spark.sparkContext.getConf().get('spark.eventLog.enabled') == 'true' else '‚ùå Disabled'}\")\n",
    "print(f\"üíæ DataFrame Cached: {'‚úÖ Yes' if df.is_cached else '‚ùå No'}\")\n",
    "print(f\"üîó Cluster Mode: {'‚úÖ Docker Cluster' if 'spark://' in spark.sparkContext.master else '‚ùå Local Mode'}\")\n",
    "\n",
    "# Unpersist cached DataFrame\n",
    "df.unpersist()\n",
    "print(f\"\\nüóëÔ∏è Cache cleared\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚è∞ Analysis completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "678e3063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Terminating Spark Session...\n",
      "‚úÖ Spark session terminated successfully\n",
      "üìã Application: DockerClusterDemo\n",
      "üÜî Application ID: app-20250903144158-0016\n",
      "\n",
      "üìÅ Event Log Files (5 total):\n",
      "   üìÑ app-20250903143443-0014.zstd (223,506 bytes)\n",
      "   üìÑ app-20250903143812-0015.zstd (224,462 bytes)\n",
      "   üìÑ app-20250903144158-0016.zstd (278,983 bytes)\n",
      "   üìÑ local-1756909626239 (189,404 bytes)\n",
      "   üìÑ local-1756909983686 (1,001,424 bytes)\n",
      "\n",
      "üîç Monitor URLs (may require application restart):\n",
      "   üìä Cluster Master: http://localhost:8080\n",
      "   üìà History Server: http://localhost:18080\n",
      "   üê≥ JupyterLab: http://localhost:8888\n",
      "\n",
      "============================================================\n",
      "üéâ Docker Cluster Demo Completed Successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SESSION TERMINATION & EVENT LOG VERIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üõë Terminating Spark Session...\")\n",
    "\n",
    "# Get session info before stopping\n",
    "app_id = spark.sparkContext.applicationId\n",
    "app_name = spark.sparkContext.appName\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "print(f\"‚úÖ Spark session terminated successfully\")\n",
    "print(f\"üìã Application: {app_name}\")\n",
    "print(f\"üÜî Application ID: {app_id}\")\n",
    "\n",
    "# Verify event logs were created\n",
    "import glob\n",
    "event_files = glob.glob(os.path.join(events_dir, \"*\"))\n",
    "print(f\"\\nüìÅ Event Log Files ({len(event_files)} total):\")\n",
    "\n",
    "for event_file in sorted(event_files):\n",
    "    file_name = os.path.basename(event_file)\n",
    "    file_size = os.path.getsize(event_file)\n",
    "    print(f\"   üìÑ {file_name} ({file_size:,} bytes)\")\n",
    "\n",
    "print(f\"\\nüîç Monitor URLs (may require application restart):\")\n",
    "print(f\"   üìä Cluster Master: http://localhost:8080\")\n",
    "print(f\"   üìà History Server: http://localhost:18080\")\n",
    "print(f\"   üê≥ JupyterLab: http://localhost:8888\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ Docker Cluster Demo Completed Successfully!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b27904",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ What We Accomplished\n",
    "\n",
    "This notebook successfully demonstrated:\n",
    "\n",
    "### ‚úÖ **Technical Achievements**\n",
    "- **Docker Cluster Integration**: External driver connecting to containerized Spark cluster\n",
    "- **Event Logging**: Comprehensive tracking with compressed storage\n",
    "- **Advanced Analytics**: Multi-dimensional data analysis with joins and aggregations\n",
    "- **Performance Optimization**: DataFrame caching and adaptive query execution\n",
    "\n",
    "### üìä **Analytics Results**\n",
    "- **Employee Dataset**: 10 records across 3 departments\n",
    "- **Department Analysis**: Engineering, Sales, Marketing statistics\n",
    "- **Salary Analysis**: Performance categorization and comparisons\n",
    "- **Demographics**: Age group distributions and cross-analysis\n",
    "\n",
    "### üîß **Infrastructure Components**\n",
    "- **Spark Master**: Cluster coordination and resource management\n",
    "- **Spark Workers**: Distributed computation execution\n",
    "- **History Server**: Event log analysis and visualization\n",
    "- **Event Storage**: Persistent logging with compression\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Next Steps\n",
    "\n",
    "1. **Scale the Dataset**: Replace sample data with real datasets\n",
    "2. **Advanced Analytics**: Implement machine learning pipelines\n",
    "3. **Performance Tuning**: Optimize cluster resources for larger workloads\n",
    "4. **Monitoring**: Set up alerting and metrics collection\n",
    "\n",
    "---\n",
    "\n",
    "## üìû Resources\n",
    "\n",
    "- **Apache Spark Documentation**: https://spark.apache.org/docs/latest/\n",
    "- **Docker Compose Reference**: https://docs.docker.com/compose/\n",
    "- **PySpark API**: https://spark.apache.org/docs/latest/api/python/\n",
    "\n",
    "---\n",
    "\n",
    "*Demo completed successfully! All events have been logged and are available in the History Server.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
