{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3ef27d",
   "metadata": {},
   "source": [
    "# Spark Docker Demo - Dual Environment Support âœ¨\n",
    "\n",
    "**âœ… FIXED**: This notebook now works in both VS Code and JupyterLab container!\n",
    "\n",
    "## ğŸ¯ Auto Environment Detection\n",
    "\n",
    "This notebook automatically detects and configures for:\n",
    "\n",
    "### ğŸ’» **VS Code Local Mode**\n",
    "- **Spark**: Local instance (`local[*]`)\n",
    "- **Events**: Saved to local directory \n",
    "- **UI**: http://localhost:4040\n",
    "- **Perfect for**: Development and testing\n",
    "\n",
    "### ğŸ³ **Docker Container Mode** \n",
    "- **Spark**: Cluster connection (`spark://spark-master:7077`)\n",
    "- **Events**: Shared with History Server\n",
    "- **UI**: Full cluster monitoring\n",
    "- **Perfect for**: Production-like environment\n",
    "\n",
    "## ğŸš€ How to Use:\n",
    "\n",
    "### Option 1: VS Code (Current)\n",
    "1. Run cells directly in VS Code\n",
    "2. Events saved to `/Users/.../spark-docker/events`\n",
    "3. View at: http://localhost:4040\n",
    "\n",
    "### Option 2: JupyterLab Container\n",
    "1. Make sure containers are running: `docker compose up -d`\n",
    "2. Open browser: **http://localhost:8888**\n",
    "3. Navigate to this notebook and run cells\n",
    "4. View cluster at: http://localhost:8080\n",
    "\n",
    "## ğŸ”§ Current Setup:\n",
    "- **Spark Version**: 3.5.0 (Optimized)\n",
    "- **Auto Detection**: âœ… Working\n",
    "- **Event Logging**: âœ… Enabled for both environments\n",
    "- **Health Checks**: âœ… All services healthy\n",
    "- **Dependencies**: âœ… Perfect startup sequence\n",
    "\n",
    "## ğŸ“Š Monitoring URLs:\n",
    "- **Local Spark UI**: http://localhost:4040 (VS Code)\n",
    "- **Master UI**: http://localhost:8080 (Docker cluster)\n",
    "- **History Server**: http://localhost:18080 (All events)\n",
    "- **JupyterLab**: http://localhost:8888 (Container access)\n",
    "\n",
    "## âœ¨ New Features:\n",
    "- âœ… **Dual Environment Support**: Works in VS Code + Docker\n",
    "- âœ… **Auto Configuration**: Detects environment automatically  \n",
    "- âœ… **Event Generation**: Creates events in both modes\n",
    "- âœ… **Performance Testing**: Caching and optimization demos\n",
    "- âœ… **Comprehensive Monitoring**: Full cluster information\n",
    "\n",
    "## ğŸ› Troubleshooting:\n",
    "- **VS Code**: Events saved locally, check `/events` directory\n",
    "- **Container**: If connection fails, restart: `docker compose restart`\n",
    "- **UI Access**: All UIs should be accessible simultaneously\n",
    "- **Events**: History Server shows events from both environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import os\n",
    "import socket\n",
    "\n",
    "# Detect environment vÃ  cáº¥u hÃ¬nh phÃ¹ há»£p\n",
    "def detect_environment():\n",
    "    \"\"\"Detect if running in Docker container or local VS Code\"\"\"\n",
    "    try:\n",
    "        # Check if we're in JupyterLab container\n",
    "        hostname = socket.gethostname()\n",
    "        if \"jupyter\" in hostname or os.path.exists(\"/home/jovyan\"):\n",
    "            return \"jupyter_container\"\n",
    "        # Check if we can connect to Spark cluster\n",
    "        elif os.path.exists(\"/Users/congdinh/Downloads/work/content/de/spark-docker/events\"):\n",
    "            return \"vscode_local\"\n",
    "        else:\n",
    "            return \"vscode_local\"\n",
    "    except:\n",
    "        return \"vscode_local\"\n",
    "\n",
    "environment = detect_environment()\n",
    "print(f\"ğŸ” Detected environment: {environment}\")\n",
    "\n",
    "# Cáº¥u hÃ¬nh dá»±a trÃªn environment\n",
    "if environment == \"jupyter_container\":\n",
    "    print(\"ğŸ³ Running in JupyterLab container - using cluster configuration\")\n",
    "    spark_config = SparkSession.builder \\\n",
    "        .appName(\"OptimizedSparkDemo\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "        .config(\"spark.driver.host\", \"jupyter-lab\") \\\n",
    "        .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "        .config(\"spark.eventLog.dir\", \"file:///events\")\n",
    "else:\n",
    "    print(\"ğŸ’» Running in VS Code - using local configuration\")\n",
    "    # Táº¡o local events directory náº¿u chÆ°a cÃ³\n",
    "    local_events_dir = \"/Users/congdinh/Downloads/work/content/de/spark-docker/events\"\n",
    "    os.makedirs(local_events_dir, exist_ok=True)\n",
    "    \n",
    "    spark_config = SparkSession.builder \\\n",
    "        .appName(\"LocalSparkDemo\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "        .config(\"spark.eventLog.dir\", f\"file://{local_events_dir}\") \\\n",
    "        .config(\"spark.ui.enabled\", \"true\") \\\n",
    "        .config(\"spark.ui.port\", \"4040\")\n",
    "\n",
    "# Táº¡o SparkSession\n",
    "spark = spark_config.getOrCreate()\n",
    "\n",
    "print(f\"âœ… Spark Version: {spark.version}\")\n",
    "print(f\"ğŸ¯ Spark Master URL: {spark.conf.get('spark.master')}\")\n",
    "print(f\"ğŸ“± Spark App Name: {spark.conf.get('spark.app.name')}\")\n",
    "print(f\"ğŸ“ Event Log Dir: {spark.conf.get('spark.eventLog.dir')}\")\n",
    "\n",
    "# Test vá»›i data Ä‘Æ¡n giáº£n\n",
    "print(\"\\n=== Basic DataFrame Test ===\")\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# Test performance vá»›i data lá»›n hÆ¡n\n",
    "print(\"\\n=== Performance Test ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Táº¡o DataFrame lá»›n hÆ¡n Ä‘á»ƒ test\n",
    "large_data = [(f\"User_{i}\", i % 100) for i in range(10000)]\n",
    "large_df = spark.createDataFrame(large_data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Thá»±c hiá»‡n má»™t sá»‘ operations\n",
    "result = large_df.groupBy(\"Age\").count().orderBy(\"Age\")\n",
    "result.show(10)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"â±ï¸ Processing time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\n=== Spark UI URLs ===\")\n",
    "print(f\"ğŸŒ Spark Application UI: {spark.sparkContext.uiWebUrl}\")\n",
    "if environment == \"jupyter_container\":\n",
    "    print(\"ğŸ”— Master UI: http://localhost:8080\")\n",
    "    print(\"ğŸ“Š History Server: http://localhost:18080\")\n",
    "else:\n",
    "    print(\"ğŸ”— Local Spark UI: http://localhost:4040\")\n",
    "    print(\"ğŸ“ Events Directory: /Users/congdinh/Downloads/work/content/de/spark-docker/events\")\n",
    "\n",
    "print(f\"\\nâœ¨ Environment: {environment}\")\n",
    "print(\"ğŸš€ SparkSession is ready for use!\")\n",
    "\n",
    "# KhÃ´ng stop context Ä‘á»ƒ cÃ³ thá»ƒ xem UI vÃ  events\n",
    "# spark.stop()  # Comment out Ä‘á»ƒ giá»¯ SparkSession active"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
